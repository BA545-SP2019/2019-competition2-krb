{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Audit Report\n",
    "\n",
    "This is the first part of our Competition 2 where we performed our preprocessing steps on the data. More details can be found in our [README.MD](README.md) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Our Required Packages and things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries and packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import backend as bk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas import ExcelWriter\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#set plot style to 'ggplot' and don't limit the view of DF when diaplyhign them to screen\n",
    "plt.style.use('ggplot')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing our data and resetting our index\n",
    "df = pd.read_excel('Data/Comp2_Raw_Data.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming and Dropping Columns\n",
    "\n",
    "To make our dataset cleaner, we renamed our columns and we also dropped the ID column because it had no value to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['ID'])\n",
    "df.columns = ['Credit_Limit', 'Gender', 'Education', 'Marriage',  'Age', 'Pay_Sept', 'Pay_Aug', 'Pay_Jul', 'Pay_Jun', 'Pay_May', 'Pay_Apr', \n",
    "             'Bill_Amt_Sept', 'Bill_Amt_Aug', 'Bill_Amt_Jul', 'Bill_Amt_Jun', 'Bill_Amt_May', 'Bill_Amt_Apr','Pay_Amt_Sept', 'Pay_Amt_Aug',\n",
    "             'Pay_Amt_Jul', 'Pay_Amt_Jun', 'Pay_Amt_May', 'Pay_Amt_Apr', 'Default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A Target DF and a Feature DF \n",
    "\n",
    "We seperated our data into a df_target which held all of our target variables. This way we don't accidently scale/transform them or include them as a feature in our feature selection/reduction  below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy our target variables to their own df\n",
    "df_target = df[['Default']].copy()\n",
    "df_target.head(10)\n",
    "#change the data type to categorical\n",
    "df_target['Default'] = pd.Categorical(df_target.Default)\n",
    "#Drop Default from our target variable df\n",
    "df = df.drop(['Default'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Our DataTypes\n",
    "\n",
    "We want to make our fields from integer to float so we do that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chnage column datatypes to float\n",
    "for col in df:\n",
    "    df[col]=pd.to_numeric(df[col], errors='coerce', downcast='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA On Our Data\n",
    "\n",
    "Here we check for missing values as well as begin our preocessing steps to transofrm and scale our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to scale and transform our continous fields. We copy these to a new dataframe so we don't impact our categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale our continous fields\n",
    "columns = ['Bill_Amt_Apr', 'Bill_Amt_May', 'Bill_Amt_Jun', 'Bill_Amt_Jul', 'Bill_Amt_Aug', 'Bill_Amt_Sept', \n",
    "           'Pay_Amt_Apr', 'Pay_Amt_May', 'Pay_Amt_Jun', 'Pay_Amt_Jul', 'Pay_Amt_Aug', 'Pay_Amt_Sept', 'Credit_Limit']\n",
    "#Copy our target variables to their own df\n",
    "df_cont = df[columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we do is handle outliers, on all of our continous columns. We move all the data to be within 3 std dev of the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_cont.columns:\n",
    "    u_bound = df_cont[col].mean() + 3* df_cont[col].std()\n",
    "    l_bound = df_cont[col].mean() - 3* df_cont[col].std()\n",
    "    df_cont[col][df_cont[col] > u_bound] = u_bound\n",
    "    df_cont[col][df_cont[col] < l_bound] = l_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we normalize our data to remove the skewness. We Use Sckitlearns `normalize` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont = pd.DataFrame(preprocessing.normalize(df_cont,norm='l2'),columns = df_cont.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `MinMaxScaler` to scale all of oour data so it is between `[0,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_cont = pd.DataFrame(min_max_scaler.fit_transform(df_cont),columns = df_cont.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_cols = ['Pay_Amt_Apr', 'Pay_Amt_May', 'Pay_Amt_Jun', 'Pay_Amt_Jul', 'Pay_Amt_Aug', 'Pay_Amt_Sept']\n",
    "for col in pay_cols:\n",
    "     df_cont[col]=np.log(df_cont[col]+.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cont.reset_index(drop=True, inplace=True)\n",
    "for col in df_cont:\n",
    "    df[col] = df_cont[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_test, y_train, y_test = train_test_split(df,df_target, test_size= 0.20, random_state=2019)\n",
    "oversample = pd.concat([X_train,y_train],axis=1)\n",
    "max_size = oversample['Default'].value_counts().max()\n",
    "lst = [oversample]\n",
    "    \n",
    "for class_index, group in oversample.groupby('Default'):\n",
    "    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "X_train = pd.concat(lst)\n",
    "y_train=pd.DataFrame.copy(X_train['Default'])\n",
    "del X_train['Default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(seed=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train,y_train)\n",
    "y_pred = xgb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.82      0.85      4710\n",
      "          1       0.48      0.61      0.54      1290\n",
      "\n",
      "avg / total       0.80      0.77      0.78      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3856  854]\n",
      " [ 504  786]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7736666666666666\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "scores = cross_val_score(xgb_model, x_test, y_test.Default.ravel(), cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to Optimze\n",
    "\n",
    "Below we try to optimze our hyperparameters using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters we want to tune\n",
    "cv_params = {'max_depth': [1,2,3,4,5,6], 'min_child_weight': [1,2,3,4], 'learning_rate': [0.1,0.2,0.3], 'n_estimators': [50,100,150]}    # parameters to be tries in the grid search\n",
    "fix_params = {'learning_rate': 0.2, 'n_estimators': 100, 'objective': 'binary:logistic'}   #other parameters, fixed for the moment \n",
    "csv = RandomizedSearchCV(xgb.XGBClassifier(**fix_params), cv_params, scoring = 'f1', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_child_weight': 4,\n",
       " 'max_depth': 6,\n",
       " 'learning_rate': 0.3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best parameters\n",
    "csv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-bwalsh/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(seed=2019, min_child_weight=4, max_depth=6, learning_rate= 0.3, n_estimators=100, objective='binary:logistic')\n",
    "xgb_model.fit(X_train,y_train)\n",
    "y_pred = xgb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.82      0.85      4710\n",
      "          1       0.47      0.57      0.52      1290\n",
      "\n",
      "avg / total       0.79      0.77      0.78      6000\n",
      "\n",
      "[[3872  838]\n",
      " [ 549  741]]\n",
      "0.7688333333333334\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "scores = cross_val_score(xgb_model, x_test, y_test.Default.ravel(), cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08562401, 0.00758294, 0.01484992, 0.00947867, 0.05876777,\n",
       "       0.01958926, 0.00726698, 0.00947867, 0.0056872 , 0.00663507,\n",
       "       0.00695103, 0.08404423, 0.0600316 , 0.06066351, 0.06477093,\n",
       "       0.05276461, 0.06192733, 0.07677725, 0.05845182, 0.07772512,\n",
       "       0.0600316 , 0.05466035, 0.05624013], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
